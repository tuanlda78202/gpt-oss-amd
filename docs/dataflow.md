# Data Flow

## Entry point

```
main() â†’ argument parsing â†’ build_transformer() â†’ read_tokenizer() â†’ build_sampler()
    â†“
Mode selection: generate|chat|getp
    â†“
getp() [if batch mode]
```

## Batch Processing

```
getp() â†’ read_inputfile() â†’ build_requests()
    â†“
warm_up() [âš ï¸ EMPTY - OPTIMIZE HERE]
    â†“
inference() â†’ for each request: simple_getp_generate()
    â†“                              â†“
finish() [âš ï¸ EMPTY]               encode() â†’ generation loop â†’ forward() â†’ sample()
    â†“                              â†“
write_outputfile()                 store output tokens
```

## Forward

```
forward() â†’ token embedding lookup
    â†“
for each layer:
    â†“
    rmsnorm() â†’ matmul(QKV) â†’ apply_rotary_emb() â†’ multi_head_attention()
    â†“                â†“                     â†“              â†“
    [norm]      [ğŸ”¥ EXPENSIVE]      [position]     [ğŸ”¥ VERY EXPENSIVE]
    â†“
    attention_output() â†’ residual_connection()
    â†“
    rmsnorm() â†’ matmul(router) â†’ topk() â†’ expert_processing()
    â†“              â†“              â†“           â†“
    [norm]    [ğŸ”¥ EXPENSIVE]   [selection]  [ğŸ”¥ VERY EXPENSIVE]
    â†“
    expert_aggregation() â†’ residual_connection()
    â†“
[repeat for all layers]
    â†“
final_rmsnorm() â†’ matmul(output) â†’ logits
                       â†“
                 [ğŸ”¥ MOST EXPENSIVE]
```

## Memory Usage

```
Model Weights: ~20GB-120GB (memory-mapped, read-only)
KV Cache: n_layers Ã— seq_len Ã— kv_dim Ã— sizeof(float) Ã— batch_size
    Example: 32 layers Ã— 4096 seq Ã— 128 kv_dim Ã— 4 bytes Ã— 8 batch = 512MB per request
Activations: ~hidden_dim Ã— multiple buffers = ~10-100MB per request
```

## Computational Hotspots

1. Matrix Multiplications (80% of time):

- QKV projections: hidden_dim Ã— (n_heads Ã— head_dim)
- Attention outputs: (n_heads Ã— head_dim) Ã— hidden_dim
- MoE projections: hidden_dim Ã— (2 Ã— intermediate_dim) per expert
- Final output: hidden_dim Ã— vocab_size

2. Attention Computation (15% of time):

- QÂ·K^T: seq_len Ã— seq_len Ã— n_heads Ã— head_dim
- AttentionÃ—V: seq_len Ã— n_heads Ã— head_dim

3. Memory Bandwidth (5% of time):

- KV cache access
- Weight loading
- Activation transfers
