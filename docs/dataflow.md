# Data Flow

- [Data Flow](#data-flow)
  - [Entry point](#entry-point)
  - [Batch Processing](#batch-processing)
  - [Forward](#forward)
  - [Memory Usage](#memory-usage)
  - [Computational Hotspots](#computational-hotspots)
  - [Visualization](#visualization)

## Entry point

```
main() â†’ argument parsing â†’ build_transformer() â†’ read_tokenizer() â†’ build_sampler()
    â†“
Mode selection: generate|chat|getp
    â†“
getp() [if batch mode]
```

## Batch Processing

```
getp() â†’ read_inputfile() â†’ build_requests()
    â†“
warm_up() [âš ï¸ EMPTY - OPTIMIZE HERE]
    â†“
inference() â†’ for each request: simple_getp_generate()
    â†“                              â†“
finish() [âš ï¸ EMPTY]               encode() â†’ generation loop â†’ forward() â†’ sample()
    â†“                              â†“
write_outputfile()                 store output tokens
```

## Forward

```
forward() â†’ token embedding lookup
    â†“
for each layer:
    â†“
    rmsnorm() â†’ matmul(QKV) â†’ apply_rotary_emb() â†’ multi_head_attention()
    â†“                â†“                     â†“              â†“
    [norm]      [ðŸ”¥ EXPENSIVE]      [position]     [ðŸ”¥ VERY EXPENSIVE]
    â†“
    attention_output() â†’ residual_connection()
    â†“
    rmsnorm() â†’ matmul(router) â†’ topk() â†’ expert_processing()
    â†“              â†“              â†“           â†“
    [norm]    [ðŸ”¥ EXPENSIVE]   [selection]  [ðŸ”¥ VERY EXPENSIVE]
    â†“
    expert_aggregation() â†’ residual_connection()
    â†“
[repeat for all layers]
    â†“
final_rmsnorm() â†’ matmul(output) â†’ logits
                       â†“
                 [ðŸ”¥ MOST EXPENSIVE]
```

## Memory Usage

```
Model Weights: ~20GB-120GB (memory-mapped, read-only)
KV Cache: n_layers Ã— seq_len Ã— kv_dim Ã— sizeof(float) Ã— batch_size
    Example: 32 layers Ã— 4096 seq Ã— 128 kv_dim Ã— 4 bytes Ã— 8 batch = 512MB per request
Activations: ~hidden_dim Ã— multiple buffers = ~10-100MB per request
```

## Computational Hotspots

1. Matrix Multiplications (80% of time):

- QKV projections: `hidden_dim Ã— (n_heads Ã— head_dim)`
- Attention outputs: `(n_heads Ã— head_dim) Ã— hidden_dim`
- MoE projections: `hidden_dim Ã— (2 Ã— intermediate_dim)` per expert
- Final output: `hidden_dim Ã— vocab_size`

2. Attention Computation (15% of time):

- QÂ·K^T: `seq_len Ã— seq_len Ã— n_heads Ã— head_dim`
- AttentionÃ—V: `seq_len Ã— n_heads Ã— head_dim`

3. Memory Bandwidth (5% of time):

- KV cache access
- Weight loading
- Activation transfers

## Visualization

1. General flow

    ```mermaid
    graph TD
        A["`**main() in run.cpp**
        Command Line Parsing
        -m getp -i input.txt -o output.txt`"] --> B["`**Build Core Components**
        1. build_transformer() - Load model weights
        2. read_tokenizer() - Load tokenizer
        3. build_sampler() - Init sampling`"]
        
        B --> C{"`**Mode Selection**`"}
        C -->|generate| D["`**generate()**
        Single prompt â†’ completion`"]
        C -->|chat| E["`**chat()**
        Interactive conversation`"]
        C -->|getp| F["`**getp() in getp_eval.cpp**
        Batch processing for throughput`"]
        
        F --> G["`**read_inputfile()**
        Load batch of input prompts
        â†’ Requests struct`"]
        
        G --> H["`**Requests Structure**
        â€¢ num_reqs: Number of prompts
        â€¢ str_reqs: Raw text prompts
        â€¢ tok_gens: Output token buffers
        â€¢ max_seq_len: Sequence limit`"]
        
        H --> I["`**ðŸ”¥ WARM-UP PHASE ðŸ”¥**
        warm_up(transformer, tokenizer)
        âš ï¸ CURRENTLY EMPTY - OPTIMIZE HERE!
        â€¢ Pre-allocate GPU memory
        â€¢ Load model to GPU
        â€¢ Initialize kernels`"]
        
        I --> J["`**âš¡ INFERENCE PHASE âš¡**
        inference(transformer, tokenizer, sampler, requests)
        ðŸ“Š TIMED FOR THROUGHPUT MEASUREMENT`"]
        
        J --> K["`**Current Sequential Processing**
        for each request:
          simple_getp_generate()`"]
        
        K --> L["`**simple_getp_generate() Flow**
        1. encode(prompt) â†’ token_ids
        2. for each position:
           â€¢ forward(transformer, token, pos)
           â€¢ sample(logits) â†’ next_token
        3. Store output tokens`"]
        
        L --> M["`**forward() - Core Transformer**
        ðŸ§  HEAVY COMPUTATION HERE
        â€¢ Token embedding lookup
        â€¢ For each layer:
          - RMSNorm â†’ Attention
          - MoE routing â†’ Expert selection
          - Residual connections
        â€¢ Output logits`"]
        
        M --> N["`**ðŸ”„ Back to Sequential Loop**
        Process next request...
        âŒ NO BATCHING
        âŒ NO GPU USAGE
        âŒ NO PARALLELISM`"]
        
        N --> O["`**write_outputfile()**
        Save generated token sequences
        to output file`"]
        
        O --> P["`**ðŸ§¹ FINISH PHASE ðŸ§¹**
        finish(transformer, tokenizer)
        âš ï¸ CURRENTLY EMPTY - CLEANUP HERE!
        â€¢ Free GPU memory
        â€¢ Unload kernels`"]
        
        P --> Q["`**Performance Report**
        ðŸ“ˆ Throughput: tokens/sec
        â±ï¸ Elapsed time`"]
    
        style I fill:#ff9999,stroke:#ff0000,stroke-width:3px
        style J fill:#ffeb99,stroke:#ff6600,stroke-width:3px
        style K fill:#ff9999,stroke:#ff0000,stroke-width:2px
        style L fill:#99ccff,stroke:#0066cc,stroke-width:2px
        style M fill:#cc99ff,stroke:#6600cc,stroke-width:3px
        style N fill:#ff9999,stroke:#ff0000,stroke-width:2px
        style P fill:#ff9999,stroke:#ff0000,stroke-width:3px
    ```

2. Pipeline flow

    ```mermaid
    graph LR
        subgraph "`**INPUT DATA FLOW**`"
            A["`**input.txt**
            N
            prompt1
            prompt2
            ...
            promptN`"] --> B["`**Requests Structure**
            ðŸ“¦ Container for batch data`"]
            
            B --> C["`**str_reqs**
            Raw text prompts
            [prompt1][prompt2]...[promptN]`"]
            
            B --> D["`**tok_gens**
            Output token buffers
            [tokens1][tokens2]...[tokensN]`"]
        end
        
        subgraph "`**CORE MODEL COMPONENTS**`"
            E["`**Transformer**
            ðŸ§  Model weights & config
            â€¢ Config: vocab_size, n_layers, etc.
            â€¢ Weights: embeddings, attention, MLP
            â€¢ State: activations, KV cache`"]
            
            F["`**Tokenizer**
            ðŸ”¤ Text â†” Token conversion
            â€¢ encode(): text â†’ token_ids
            â€¢ decode(): token_ids â†’ text`"]
            
            G["`**Sampler**
            ðŸŽ² Token selection
            â€¢ Temperature, top-p
            â€¢ Random number generation`"]
        end
        
        subgraph "`**PROCESSING PIPELINE**`"
            H["`**FOR EACH REQUEST:**
            1. Get str_req[i]`"] --> I["`**encode()**
            text â†’ token_ids
            [101, 2023, 345, ...]`"]
            
            I --> J["`**Generation Loop**
            pos = 0 to max_seq_len`"]
            
            J --> K["`**forward()**
            ðŸ”¥ MOST EXPENSIVE
            token, pos â†’ logits
            [0.1, 0.05, 0.8, ...]`"]
            
            K --> L["`**sample()**
            logits â†’ next_token
            select based on temperature/top-p`"]
            
            L --> M["`**Store in tok_gens[i]**
            Accumulate output tokens`"]
            
            M --> N{"`More positions?`"}
            N -->|Yes| J
            N -->|No| O["`**Next Request**`"]
            O --> H
        end
        
        subgraph "`**OPTIMIZATION TARGETS**`"
            P["`**âŒ CURRENT BOTTLENECKS**
            â€¢ Sequential request processing
            â€¢ CPU-only computation
            â€¢ No memory reuse
            â€¢ Single-threaded forward()`"]
            
            Q["`**âœ… OPTIMIZATION OPPORTUNITIES**
            â€¢ Batch multiple requests
            â€¢ GPU acceleration (HIP kernels)
            â€¢ Multi-GPU parallelism
            â€¢ Efficient KV cache management
            â€¢ OpenMP threading`"]
        end
        
        subgraph "`**OUTPUT**`"
            R["`**output.txt**
            token_id1 token_id2 ... \\n
            token_id1 token_id2 ... \\n
            ...`"] --> S["`**decode (separate tool)**
            Convert token IDs back to text
            for human reading`"]
        end
        
        C --> H
        D --> M
        E --> K
        F --> I
        G --> L
        M --> R
        
        style P fill:#ff9999,stroke:#ff0000,stroke-width:2px
        style Q fill:#99ff99,stroke:#00cc00,stroke-width:2px
        style K fill:#ffcc99,stroke:#ff6600,stroke-width:3px
    ```

3. Model architecture

    ```mermaid
    graph TD
        A["`**Input Token**
        token_id (e.g., 1337)`"] --> B["`**Token Embedding**
        embedding_table[token_id]
        â†’ hidden_dim vector`"]
        
        B --> C["`**Layer Loop**
        for l = 0 to n_layers-1`"]
        
        C --> D["`**RMSNorm (Attention)**
        Normalize activations`"]
        
        D --> E["`**QKV Projection**
        ðŸ”¥ EXPENSIVE: matmul + bias
        hidden_dim â†’ (q,k,v) heads`"]
        
        E --> F["`**RoPE (Rotary Embedding)**
        Apply position encoding
        with YaRN scaling`"]
        
        F --> G["`**Multi-Head Attention**
        ðŸ”¥ VERY EXPENSIVE
        â€¢ QÂ·K^T attention scores
        â€¢ Softmax normalization  
        â€¢ Weighted V aggregation
        â€¢ Grouped Query Attention (GQA)`"]
        
        G --> H["`**Attention Output**
        matmul + bias + residual`"]
        
        H --> I["`**RMSNorm (MLP)**
        Normalize for feed-forward`"]
        
        I --> J["`**MoE Router**
        ðŸ”¥ EXPENSIVE: matmul + bias
        Select top-k experts`"]
        
        J --> K["`**Expert Processing**
        For each selected expert:
        ðŸ”¥ VERY EXPENSIVE
        â€¢ Gate/Up projection (2Ã—intermediate_dim)
        â€¢ SwiGLU activation
        â€¢ Down projection`"]
        
        K --> L["`**Expert Aggregation**
        Weighted sum of expert outputs
        + residual connection`"]
        
        L --> M{"`More layers?`"}
        M -->|Yes| C
        M -->|No| N["`**Final RMSNorm**
        Normalize final activations`"]
        
        N --> O["`**Output Projection**
        ðŸ”¥ EXPENSIVE: matmul
        hidden_dim â†’ vocab_size logits`"]
        
        O --> P["`**Sampling**
        â€¢ Apply temperature
        â€¢ Softmax â†’ probabilities
        â€¢ Top-p or argmax selection`"]
        
        subgraph "`**Memory Systems**`"
            Q["`**KV Cache**
        key_cache[layer][pos][kv_dim]
        value_cache[layer][pos][kv_dim]
        ðŸ”¥ HUGE MEMORY USAGE`"]
            
            R["`**Activations**
            â€¢ x: current activations
            â€¢ qkv: attention projections  
            â€¢ expert buffers
            ðŸ”¥ TEMPORARY MEMORY`"]
        end
        
        subgraph "`**Computational Hotspots**`"
            S["`**ðŸ”¥ Matrix Multiplications**
            â€¢ QKV projections
            â€¢ Attention output
            â€¢ MoE gate/up/down
            â€¢ Final output
            âš¡ GPU ACCELERATION TARGET`"]
            
            T["`**ðŸ”¥ Attention Computation**
            â€¢ QÂ·K^T (sequence Ã— heads)
            â€¢ Attention Ã— V
            âš¡ MEMORY BANDWIDTH BOUND`"]
            
            U["`**ðŸ”¥ Expert Selection**
            â€¢ Router computation
            â€¢ Top-k sorting
            â€¢ Conditional execution
            âš¡ BATCHING OPPORTUNITY`"]
        end
        
        F --> Q
        G --> Q
        E --> R
        K --> R
        
        style S fill:#ff6666,stroke:#cc0000,stroke-width:3px
        style T fill:#ff6666,stroke:#cc0000,stroke-width:3px  
        style U fill:#ff6666,stroke:#cc0000,stroke-width:3px
        style Q fill:#ffcc99,stroke:#ff6600,stroke-width:2px
        style R fill:#ffcc99,stroke:#ff6600,stroke-width:2px
    ```
