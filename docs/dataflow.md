# Data Flow

- [Data Flow](#data-flow)
  - [Entry point](#entry-point)
  - [Batch Processing](#batch-processing)
  - [Forward](#forward)
  - [Memory Usage](#memory-usage)
  - [Computational Hotspots](#computational-hotspots)
  - [Visualization](#visualization)

## Entry point

```
main() â†’ argument parsing â†’ build_transformer() â†’ read_tokenizer() â†’ build_sampler()
    â†“
Mode selection: generate|chat|getp
    â†“
getp() [if batch mode]
```

## Batch Processing

```
getp() â†’ read_inputfile() â†’ build_requests()
    â†“
warm_up() [âš ï¸ EMPTY - OPTIMIZE HERE]
    â†“
inference() â†’ for each request: simple_getp_generate()
    â†“                              â†“
finish() [âš ï¸ EMPTY]               encode() â†’ generation loop â†’ forward() â†’ sample()
    â†“                              â†“
write_outputfile()                 store output tokens
```

## Forward

```
forward() â†’ token embedding lookup
    â†“
for each layer:
    â†“
    rmsnorm() â†’ matmul(QKV) â†’ apply_rotary_emb() â†’ multi_head_attention()
    â†“                â†“                     â†“              â†“
    [norm]      [ðŸ”¥ EXPENSIVE]      [position]     [ðŸ”¥ VERY EXPENSIVE]
    â†“
    attention_output() â†’ residual_connection()
    â†“
    rmsnorm() â†’ matmul(router) â†’ topk() â†’ expert_processing()
    â†“              â†“              â†“           â†“
    [norm]    [ðŸ”¥ EXPENSIVE]   [selection]  [ðŸ”¥ VERY EXPENSIVE]
    â†“
    expert_aggregation() â†’ residual_connection()
    â†“
[repeat for all layers]
    â†“
final_rmsnorm() â†’ matmul(output) â†’ logits
                       â†“
                 [ðŸ”¥ MOST EXPENSIVE]
```

## Memory Usage

```
Model Weights: ~20GB-120GB (memory-mapped, read-only)
KV Cache: n_layers Ã— seq_len Ã— kv_dim Ã— sizeof(float) Ã— batch_size
    Example: 32 layers Ã— 4096 seq Ã— 128 kv_dim Ã— 4 bytes Ã— 8 batch = 512MB per request
Activations: ~hidden_dim Ã— multiple buffers = ~10-100MB per request
```

## Computational Hotspots

1. Matrix Multiplications (80% of time):

- QKV projections: `hidden_dim Ã— (n_heads Ã— head_dim)`
- Attention outputs: `(n_heads Ã— head_dim) Ã— hidden_dim`
- MoE projections: `hidden_dim Ã— (2 Ã— intermediate_dim)` per expert
- Final output: `hidden_dim Ã— vocab_size`

2. Attention Computation (15% of time):

- QÂ·K^T: `seq_len Ã— seq_len Ã— n_heads Ã— head_dim`
- AttentionÃ—V: `seq_len Ã— n_heads Ã— head_dim`

3. Memory Bandwidth (5% of time):

- KV cache access
- Weight loading
- Activation transfers

## Modularization

```mermaid
graph TD
    %% Entry Point
    Start([Program Start]) --> Main[main - run.cpp:1099]

    %% Initialization Phase
    Main --> ParseArgs[Parse Command Line Arguments]
    ParseArgs --> InitTransformer[build_transformer - run.cpp:285]
    ParseArgs --> InitTokenizer[read_tokenizer - tokenizer.cpp:49]
    ParseArgs --> InitSampler[build_sampler - sampler.cpp:build_sampler_oss]

    %% Transformer Initialization
    InitTransformer --> LoadCheckpoint[load_checkpoint - run.cpp]
    LoadCheckpoint --> MemoryMapWeights[memory_map_weights - run.cpp]
    LoadCheckpoint --> MallocRunState[malloc_run_state - run.cpp]

    %% Mode Selection
    InitTransformer --> ModeCheck{Mode Selection}
    InitTokenizer --> ModeCheck
    InitSampler --> ModeCheck

    %% Three Execution Modes
    ModeCheck -->|"mode=='generate'"| GenerateMode[generate - run.cpp:886]
    ModeCheck -->|"mode=='chat'"| ChatMode[chat - run.cpp:985]
    ModeCheck -->|"mode=='getp'"| GetpMode[getp - getp_run.cpp]

    %% Generate Mode Flow
    GenerateMode --> EncodePrompt1[encode - tokenizer.cpp:encode]
    EncodePrompt1 --> GenLoop{Generation Loop}
    GenLoop --> ForwardPass1[forward_cpu - forward.cpp:204]
    ForwardPass1 --> Sample1[sample_oss - sampler.cpp:sample_oss]
    Sample1 --> Decode1[decode_piece - tokenizer.cpp:decode_piece]
    Decode1 --> PrintToken1[Print Token]
    PrintToken1 --> CheckSteps1{Steps < Max?}
    CheckSteps1 -->|Yes| GenLoop
    CheckSteps1 -->|No| Cleanup

    %% Chat Mode Flow
    ChatMode --> ReadInput[Read User Input]
    ReadInput --> FormatPrompt[Format Chat Prompt]
    FormatPrompt --> EncodePrompt2[encode - tokenizer.cpp:encode]
    EncodePrompt2 --> ChatLoop{Chat Generation Loop}
    ChatLoop --> ForwardPass2[forward_cpu - forward.cpp:204]
    ForwardPass2 --> Sample2[sample_oss - sampler.cpp:sample_oss]
    Sample2 --> Decode2[decode_piece - tokenizer.cpp:decode_piece]
    Decode2 --> PrintToken2[Print Token]
    PrintToken2 --> CheckSteps2{Steps < Max?}
    CheckSteps2 -->|Yes| ChatLoop
    CheckSteps2 -->|No| ReadInput

    %% Getp Mode Flow
    GetpMode --> WarmUp[warm_up - getp_run.cpp:13]
    WarmUp --> GetpEval[getp_eval - getp_eval.cpp]
    GetpEval --> SimpleGenerate[simple_getp_generate - getp_run.cpp:31]
    SimpleGenerate --> EncodePrompt3[encode - tokenizer.cpp:encode]
    EncodePrompt3 --> GetpLoop{Getp Generation Loop}
    GetpLoop --> ForwardPass3[forward_cpu - forward.cpp:204]
    ForwardPass3 --> Sample3[sample_oss - sampler.cpp:sample_oss]
    Sample3 --> StoreToken[Store Output Token]
    StoreToken --> CheckSteps3{Steps < Max?}
    CheckSteps3 -->|Yes| GetpLoop
    CheckSteps3 -->|No| FinishGetp[finish - getp_run.cpp:22]
    FinishGetp --> Cleanup

    %% Forward Pass Detail (Core Inference)
    ForwardPass1 --> TokenEmbedding[Token Embedding Lookup]
    ForwardPass2 --> TokenEmbedding
    ForwardPass3 --> TokenEmbedding

    TokenEmbedding --> LayerLoop{For Each Layer}
    LayerLoop --> AttentionNorm[rmsnorm_cpu - forward.cpp:10]
    AttentionNorm --> QKVProjection[QKV Projection - matmul_cpu]
    QKVProjection --> RoPE[apply_rotary_emb_cpu - forward.cpp:37]
    RoPE --> MultiHeadAttn[Multi-Head Attention]

    MultiHeadAttn --> AttentionCalc[Attention Score Calculation]
    AttentionCalc --> SoftmaxAttn[softmax_cpu - forward.cpp:26]
    SoftmaxAttn --> ValueAggregation[Weighted Value Aggregation]
    ValueAggregation --> OutputProjection[Output Projection - matmul_cpu]
    OutputProjection --> ResidualAdd1[Residual Connection]

    ResidualAdd1 --> FFNNorm[rmsnorm_cpu - MLP norm]
    FFNNorm --> Router[Router Network - matmul_cpu]
    Router --> TopK[topk_cpu - forward.cpp:20]
    TopK --> ExpertSelection{For Each Selected Expert}

    ExpertSelection --> MLP1[MLP1 Projection - matmul_cpu]
    MLP1 --> SwiGLU[SwiGLU Activation]
    SwiGLU --> MLP2[MLP2 Projection - matmul_cpu]
    MLP2 --> ExpertWeight[Apply Expert Weight]
    ExpertWeight --> ExpertAgg[Aggregate Expert Outputs]
    ExpertAgg --> ResidualAdd2[Residual Connection]

    ResidualAdd2 --> NextLayer{More Layers?}
    NextLayer -->|Yes| LayerLoop
    NextLayer -->|No| FinalNorm[Final rmsnorm_cpu]
    FinalNorm --> Classifier[Classifier - matmul_cpu]
    Classifier --> ReturnLogits[Return Logits]

    %% Sampling Detail
    Sample1 --> SampleType{Sampling Type}
    Sample2 --> SampleType
    Sample3 --> SampleType

    SampleType -->|"temperature=0"| ArgMax[sample_argmax_oss - sampler.cpp:9]
    SampleType -->|"topp>0"| TopP[sample_topp_oss - sampler.cpp:45]
    SampleType -->|"temperature>0"| Multinomial[sample_mult_oss - sampler.cpp:22]

    TopP --> SortProbs[Sort Probabilities]
    SortProbs --> CumulativeSum[Cumulative Sum]
    CumulativeSum --> SelectToken[Select Token]

    ArgMax --> SelectToken
    Multinomial --> SelectToken
    SelectToken --> ReturnToken[Return Token ID]

    %% Tokenization Detail
    EncodePrompt1 --> FindTokens[find_token_id - tokenizer.cpp:17]
    EncodePrompt2 --> FindTokens
    EncodePrompt3 --> FindTokens
    FindTokens --> BPEEncode[encode_piece_bytes_bpe - tokenizer.cpp:37]
    BPEEncode --> TokenArray[Return Token Array]

    Decode1 --> TokenLookup[Token to String Lookup]
    Decode2 --> TokenLookup
    TokenLookup --> SafePrint[safe_printf - tokenizer.cpp:44]

    %% Cleanup
    Cleanup --> FreeSampler[free_sampler - sampler.cpp]
    FreeSampler --> FreeTokenizer[free_tokenizer - tokenizer.cpp]
    FreeTokenizer --> FreeTransformer[free_transformer - run.cpp]
    FreeTransformer --> End([Program End])

    %% Additional Components
    DecodeStandalone[decode.cpp - Standalone Token Decoder]
    DecodeStandalone --> DecodeMain[main - decode.cpp:22]
    DecodeMain --> ReadTokenFile[Read Token File]
    ReadTokenFile --> DecodeTokens[Decode Each Token]
    DecodeTokens --> PrintDecoded[Print Decoded Text]

    %% Styling
    classDef mainFlow fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    classDef forward fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef sampling fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef tokenizer fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
    classDef modes fill:#fff8e1,stroke:#f57f17,stroke-width:2px

    class Main,ParseArgs,InitTransformer,InitTokenizer,InitSampler,ModeCheck,Cleanup mainFlow
    class ForwardPass1,ForwardPass2,ForwardPass3,TokenEmbedding,LayerLoop,AttentionNorm,MultiHeadAttn,FFNNorm,Router,TopK,FinalNorm forward
    class Sample1,Sample2,Sample3,SampleType,ArgMax,TopP,Multinomial sampling
    class EncodePrompt1,EncodePrompt2,EncodePrompt3,FindTokens,BPEEncode,Decode1,Decode2,TokenLookup tokenizer
    class GenerateMode,ChatMode,GetpMode modes
```

### Legacy

1. General flow

   ```mermaid
   graph TD
       A["`**main() in run.cpp**
       Command Line Parsing
       -m getp -i input.txt -o output.txt`"] --> B["`**Build Core Components**
       1. build_transformer() - Load model weights
       2. read_tokenizer() - Load tokenizer
       3. build_sampler() - Init sampling`"]

       B --> C{"`**Mode Selection**`"}
       C -->|generate| D["`**generate()**
       Single prompt â†’ completion`"]
       C -->|chat| E["`**chat()**
       Interactive conversation`"]
       C -->|getp| F["`**getp() in getp_eval.cpp**
       Batch processing for throughput`"]

       F --> G["`**read_inputfile()**
       Load batch of input prompts
       â†’ Requests struct`"]

       G --> H["`**Requests Structure**
       â€¢ num_reqs: Number of prompts
       â€¢ str_reqs: Raw text prompts
       â€¢ tok_gens: Output token buffers
       â€¢ max_seq_len: Sequence limit`"]

       H --> I["`**ðŸ”¥ WARM-UP PHASE ðŸ”¥**
       warm_up(transformer, tokenizer)
       âš ï¸ CURRENTLY EMPTY - OPTIMIZE HERE!
       â€¢ Pre-allocate GPU memory
       â€¢ Load model to GPU
       â€¢ Initialize kernels`"]

       I --> J["`**âš¡ INFERENCE PHASE âš¡**
       inference(transformer, tokenizer, sampler, requests)
       ðŸ“Š TIMED FOR THROUGHPUT MEASUREMENT`"]

       J --> K["`**Current Sequential Processing**
       for each request:
         simple_getp_generate()`"]

       K --> L["`**simple_getp_generate() Flow**
       1. encode(prompt) â†’ token_ids
       2. for each position:
          â€¢ forward(transformer, token, pos)
          â€¢ sample(logits) â†’ next_token
       3. Store output tokens`"]

       L --> M["`**forward() - Core Transformer**
       ðŸ§  HEAVY COMPUTATION HERE
       â€¢ Token embedding lookup
       â€¢ For each layer:
         - RMSNorm â†’ Attention
         - MoE routing â†’ Expert selection
         - Residual connections
       â€¢ Output logits`"]

       M --> N["`**ðŸ”„ Back to Sequential Loop**
       Process next request...
       âŒ NO BATCHING
       âŒ NO GPU USAGE
       âŒ NO PARALLELISM`"]

       N --> O["`**write_outputfile()**
       Save generated token sequences
       to output file`"]

       O --> P["`**ðŸ§¹ FINISH PHASE ðŸ§¹**
       finish(transformer, tokenizer)
       âš ï¸ CURRENTLY EMPTY - CLEANUP HERE!
       â€¢ Free GPU memory
       â€¢ Unload kernels`"]

       P --> Q["`**Performance Report**
       ðŸ“ˆ Throughput: tokens/sec
       â±ï¸ Elapsed time`"]

       style I fill:#ff9999,stroke:#ff0000,stroke-width:3px
       style J fill:#ffeb99,stroke:#ff6600,stroke-width:3px
       style K fill:#ff9999,stroke:#ff0000,stroke-width:2px
       style L fill:#99ccff,stroke:#0066cc,stroke-width:2px
       style M fill:#cc99ff,stroke:#6600cc,stroke-width:3px
       style N fill:#ff9999,stroke:#ff0000,stroke-width:2px
       style P fill:#ff9999,stroke:#ff0000,stroke-width:3px
   ```

2. Pipeline flow

   ```mermaid
   graph LR
       subgraph "`**INPUT DATA FLOW**`"
           A["`**input.txt**
           N
           prompt1
           prompt2
           ...
           promptN`"] --> B["`**Requests Structure**
           ðŸ“¦ Container for batch data`"]

           B --> C["`**str_reqs**
           Raw text prompts
           [prompt1][prompt2]...[promptN]`"]

           B --> D["`**tok_gens**
           Output token buffers
           [tokens1][tokens2]...[tokensN]`"]
       end

       subgraph "`**CORE MODEL COMPONENTS**`"
           E["`**Transformer**
           ðŸ§  Model weights & config
           â€¢ Config: vocab_size, n_layers, etc.
           â€¢ Weights: embeddings, attention, MLP
           â€¢ State: activations, KV cache`"]

           F["`**Tokenizer**
           ðŸ”¤ Text â†” Token conversion
           â€¢ encode(): text â†’ token_ids
           â€¢ decode(): token_ids â†’ text`"]

           G["`**Sampler**
           ðŸŽ² Token selection
           â€¢ Temperature, top-p
           â€¢ Random number generation`"]
       end

       subgraph "`**PROCESSING PIPELINE**`"
           H["`**FOR EACH REQUEST:**
           1. Get str_req[i]`"] --> I["`**encode()**
           text â†’ token_ids
           [101, 2023, 345, ...]`"]

           I --> J["`**Generation Loop**
           pos = 0 to max_seq_len`"]

           J --> K["`**forward()**
           ðŸ”¥ MOST EXPENSIVE
           token, pos â†’ logits
           [0.1, 0.05, 0.8, ...]`"]

           K --> L["`**sample()**
           logits â†’ next_token
           select based on temperature/top-p`"]

           L --> M["`**Store in tok_gens[i]**
           Accumulate output tokens`"]

           M --> N{"`More positions?`"}
           N -->|Yes| J
           N -->|No| O["`**Next Request**`"]
           O --> H
       end

       subgraph "`**OPTIMIZATION TARGETS**`"
           P["`**âŒ CURRENT BOTTLENECKS**
           â€¢ Sequential request processing
           â€¢ CPU-only computation
           â€¢ No memory reuse
           â€¢ Single-threaded forward()`"]

           Q["`**âœ… OPTIMIZATION OPPORTUNITIES**
           â€¢ Batch multiple requests
           â€¢ GPU acceleration (HIP kernels)
           â€¢ Multi-GPU parallelism
           â€¢ Efficient KV cache management
           â€¢ OpenMP threading`"]
       end

       subgraph "`**OUTPUT**`"
           R["`**output.txt**
           token_id1 token_id2 ... \\n
           token_id1 token_id2 ... \\n
           ...`"] --> S["`**decode (separate tool)**
           Convert token IDs back to text
           for human reading`"]
       end

       C --> H
       D --> M
       E --> K
       F --> I
       G --> L
       M --> R

       style P fill:#ff9999,stroke:#ff0000,stroke-width:2px
       style Q fill:#99ff99,stroke:#00cc00,stroke-width:2px
       style K fill:#ffcc99,stroke:#ff6600,stroke-width:3px
   ```

3. Model architecture

   ```mermaid
   graph TD
       A["`**Input Token**
       token_id (e.g., 1337)`"] --> B["`**Token Embedding**
       embedding_table[token_id]
       â†’ hidden_dim vector`"]

       B --> C["`**Layer Loop**
       for l = 0 to n_layers-1`"]

       C --> D["`**RMSNorm (Attention)**
       Normalize activations`"]

       D --> E["`**QKV Projection**
       ðŸ”¥ EXPENSIVE: matmul + bias
       hidden_dim â†’ (q,k,v) heads`"]

       E --> F["`**RoPE (Rotary Embedding)**
       Apply position encoding
       with YaRN scaling`"]

       F --> G["`**Multi-Head Attention**
       ðŸ”¥ VERY EXPENSIVE
       â€¢ QÂ·K^T attention scores
       â€¢ Softmax normalization
       â€¢ Weighted V aggregation
       â€¢ Grouped Query Attention (GQA)`"]

       G --> H["`**Attention Output**
       matmul + bias + residual`"]

       H --> I["`**RMSNorm (MLP)**
       Normalize for feed-forward`"]

       I --> J["`**MoE Router**
       ðŸ”¥ EXPENSIVE: matmul + bias
       Select top-k experts`"]

       J --> K["`**Expert Processing**
       For each selected expert:
       ðŸ”¥ VERY EXPENSIVE
       â€¢ Gate/Up projection (2Ã—intermediate_dim)
       â€¢ SwiGLU activation
       â€¢ Down projection`"]

       K --> L["`**Expert Aggregation**
       Weighted sum of expert outputs
       + residual connection`"]

       L --> M{"`More layers?`"}
       M -->|Yes| C
       M -->|No| N["`**Final RMSNorm**
       Normalize final activations`"]

       N --> O["`**Output Projection**
       ðŸ”¥ EXPENSIVE: matmul
       hidden_dim â†’ vocab_size logits`"]

       O --> P["`**Sampling**
       â€¢ Apply temperature
       â€¢ Softmax â†’ probabilities
       â€¢ Top-p or argmax selection`"]

       subgraph "`**Memory Systems**`"
           Q["`**KV Cache**
       key_cache[layer][pos][kv_dim]
       value_cache[layer][pos][kv_dim]
       ðŸ”¥ HUGE MEMORY USAGE`"]

           R["`**Activations**
           â€¢ x: current activations
           â€¢ qkv: attention projections
           â€¢ expert buffers
           ðŸ”¥ TEMPORARY MEMORY`"]
       end

       subgraph "`**Computational Hotspots**`"
           S["`**ðŸ”¥ Matrix Multiplications**
           â€¢ QKV projections
           â€¢ Attention output
           â€¢ MoE gate/up/down
           â€¢ Final output
           âš¡ GPU ACCELERATION TARGET`"]

           T["`**ðŸ”¥ Attention Computation**
           â€¢ QÂ·K^T (sequence Ã— heads)
           â€¢ Attention Ã— V
           âš¡ MEMORY BANDWIDTH BOUND`"]

           U["`**ðŸ”¥ Expert Selection**
           â€¢ Router computation
           â€¢ Top-k sorting
           â€¢ Conditional execution
           âš¡ BATCHING OPPORTUNITY`"]
       end

       F --> Q
       G --> Q
       E --> R
       K --> R

       style S fill:#ff6666,stroke:#cc0000,stroke-width:3px
       style T fill:#ff6666,stroke:#cc0000,stroke-width:3px
       style U fill:#ff6666,stroke:#cc0000,stroke-width:3px
       style Q fill:#ffcc99,stroke:#ff6600,stroke-width:2px
       style R fill:#ffcc99,stroke:#ff6600,stroke-width:2px
   ```
